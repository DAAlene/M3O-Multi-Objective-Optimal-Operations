README FQI

FQI learns a policy from a tuple dataset generated by a proper Design of Experiments implemented in the run_doe.m function.

DEFAULT SETTINGS:

- DOE is performed via No_exp = 50 simulations with random release decisions using a non-uniform sampling to ensure generation of “interesting” tuples. In particular, a decision between 0 and 60% of the maximum release with probability equal 0.2,  a decision between 60% of the maximum release and the maximum release with probability equal to 0.2, and a decision equal to the water demand with probability equal to 0.6 are considered.

- FQI settings are as follows: number of trees M = 200, minimum number of points per leaf nmin = 25, maximum number of iterations maxIter = 40 

- in order to explore the Pareto front, different weights combinations are used.
Specifically: lambda = [1 0; .75 .25; .5 .5 ; .35 .65; .2 .8; .1 .9;  0 1]


SUGGESTIONS FOR MORE COMPLEX PROBLEMS:

To solve more complex applications we recommend increasing the number of examples in DOE to learn the policy over a larger dataset. This will probably lead to adjustments in FQI settings, in particular higher number of trees (M > 300) and a larger number of iterations dependent on the objective function considered. The value of nmin is instead strongly case-study dependent and should be adjusted depending on the specific features of the problem.
Also the weights combinations used for the aggregation of the objectives can be changed, in order to explore the Pareto front.